[
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Part 2",
    "section": "",
    "text": "In this part we will create a sample to investigate the relation between yearly changes in firm profitability and contemporaneous returns. We will find that the relation is far from simple. In addition to being the center of many interesting questions, this analysis is a nice exercise. It allows us to go into more details on how to transform data–especially how to prepare panel data for analyses. We will compute more involved variables, such as buy-and-hold-returns. We will also learn how to combine data in multiple tables. Finally, we will discuss a few presentation design principles that are important for presenting results. The final result of our analysis will be the following plot:"
  },
  {
    "objectID": "transform.html#summary",
    "href": "transform.html#summary",
    "title": "Part 2",
    "section": "",
    "text": "In this part we will create a sample to investigate the relation between yearly changes in firm profitability and contemporaneous returns. We will find that the relation is far from simple. In addition to being the center of many interesting questions, this analysis is a nice exercise. It allows us to go into more details on how to transform data–especially how to prepare panel data for analyses. We will compute more involved variables, such as buy-and-hold-returns. We will also learn how to combine data in multiple tables. Finally, we will discuss a few presentation design principles that are important for presenting results. The final result of our analysis will be the following plot:"
  },
  {
    "objectID": "transform.html#loading-packages",
    "href": "transform.html#loading-packages",
    "title": "Part 2",
    "section": "Loading packages",
    "text": "Loading packages\n\nlibrary(collapse)\nlibrary(tidyverse)\nlibrary(ggdensity)\n1as_yearmon &lt;- zoo::as.yearmon\n\n\n1\n\nThis statement is only attaching one function as.yearmon from the {zoo} package."
  },
  {
    "objectID": "transform.html#the-grammar-of-data-manipulation",
    "href": "transform.html#the-grammar-of-data-manipulation",
    "title": "Part 2",
    "section": "The grammar of data manipulation",
    "text": "The grammar of data manipulation\nR—and especially newer packages included in the tidyverse package ecosystem—have very expressive data verbs that make code readable. Most data transformation steps concerning data tables are really a combination of a few basic actions. The most common are listed below with their names as used in R’s dplyr package. We will use these in what follows to transform the raw compustat data into the variables we need for later analysis.\n\n\n\nFigure 1: The basic grammar of data manipulation. Image source\n\n\n\ndplyr::select() picks columns of a table based on their names.\ndplyr::filter() picks rows of a table based on their values.\ndplyr::arrange() changes the ordering of the rows.\ndplyr::mutate() adds new columns that are functions of existing columns\ndplyr::summarize() reduces/aggregates multiple rows down to a single summary.\n\nIf you add join and grouping actions to this list, then 98% of everything you want to do is a combination of the above actions.\nFor example, the count function that we encountered in the previous part, is really grouped summarize action:\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\ncount(iris, Species, sort = TRUE)\n\n     Species  n\n1     setosa 50\n2 versicolor 50\n3  virginica 50\n\n\n\niris |&gt; \n1  summarize(.by = Species,\n2    n_obs = n()\n  ) |&gt; \n3  arrange(-n_obs)\n\n\n1\n\nAggregate (summarize) the data in iris by species\n\n2\n\nCompute the number of obs (in a fyear group) and call that column “n_obs”\n\n3\n\nSort the resulting tibble in descending order (the ‘-’) of n_obs\n\n\n\n\n     Species n_obs\n1     setosa    50\n2 versicolor    50\n3  virginica    50"
  },
  {
    "objectID": "transform.html#summarizing-data",
    "href": "transform.html#summarizing-data",
    "title": "Part 2",
    "section": "Summarizing data",
    "text": "Summarizing data\n\niris |&gt; \n  summarize(\n    mean_Petal.Width = mean(Petal.Width),\n    n_obs = n(),\n    stdev_Petal.Width = sd(Petal.Width)\n  )\n\n  mean_Petal.Width n_obs stdev_Petal.Width\n1         1.199333   150         0.7622377\n\n\n\niris |&gt; \n  summarize( .by = Species,\n    mean_Petal.Width = mean(Petal.Width, na.rm = TRUE),\n    stdev_Petal.Width = sd(Petal.Width, na.rm = TRUE),\n    n_miss_Petal.Width = sum(is.na(Petal.Width))\n  )\n\n     Species mean_Petal.Width stdev_Petal.Width n_miss_Petal.Width\n1     setosa            0.246         0.1053856                  0\n2 versicolor            1.326         0.1977527                  0\n3  virginica            2.026         0.2746501                  0"
  },
  {
    "objectID": "transform.html#picking-up-from-part-1",
    "href": "transform.html#picking-up-from-part-1",
    "title": "Part 2",
    "section": "Picking up from Part 1",
    "text": "Picking up from Part 1\nLet us load the data from previous chapter\n\nccm &lt;- readRDS(\"data/ccm-unique.rds\")\n\nWe will also use a second set of data this time. This is monthly stock return data from CRSP.\n\ncrsp_raw &lt;- read_csv(\"data/crsp-raw-2023-07-08-csv.zip\", show_col_types = FALSE)\n# making all column names lowercase:\ncolnames(crsp_raw) &lt;- tolower(colnames(crsp_raw))\n\nHere is how the raw returns data looks like:\n\nglimpse(crsp_raw)\n\nRows: 3,800,162\nColumns: 13\n$ permno   &lt;dbl&gt; 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000…\n$ date     &lt;date&gt; 1985-12-31, 1986-01-31, 1986-02-28, 1986-03-31, 1986-04-30, …\n$ shrcd    &lt;dbl&gt; NA, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ exchcd   &lt;dbl&gt; NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, NA,…\n$ shrcls   &lt;chr&gt; NA, \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ primexch &lt;chr&gt; NA, \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q\", \"Q…\n$ hexcd    &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2…\n$ dlstcd   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dlret    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ prc      &lt;dbl&gt; NA, -4.37500, -3.25000, -4.43750, -4.00000, -3.10938, -3.0937…\n$ ret      &lt;chr&gt; NA, \"C\", \"-0.257143\", \"0.365385\", \"-0.098592\", \"-0.222656\", \"…\n$ shrout   &lt;dbl&gt; NA, 3680, 3680, 3680, 3793, 3793, 3793, 3793, 3793, 3793, 384…\n$ vwretd   &lt;dbl&gt; 0.043061, 0.009830, 0.072501, 0.053887, -0.007903, 0.050847, …"
  },
  {
    "objectID": "transform.html#cleaning-return-variables",
    "href": "transform.html#cleaning-return-variables",
    "title": "Part 2",
    "section": "Cleaning return variables",
    "text": "Cleaning return variables\nTo generate the return variables, we first need to do some filtering. We only want common shares and only shares listed on the the three biggest US exchanges\n\ncrsp &lt;- crsp_raw |&gt; \n  filter(\n    shrcd %in% c(10, 11),  # restrict to common ordinary shares\n    exchcd %in% c(1, 2, 3)  # 1,2, and 3 are NSYE, AMEX, NASDAQ\n  ) |&gt; \n  select(-shrcd, -primexch, -hexcd, -exchcd) |&gt; \n  arrange(permno, date)\n\nThe output of glimpse above also showed you that two columns we need next (monthly returns ret and delisting returns dlret) contain a weird mix of character and number values. The character values encode some situations in a month, that we do not care about right now. We will set these values to missing so that we can turn both columns into numeric columns. We use an dplyr::if_else function to do so. It has the form: “if the if condition is true, use this value, else use that value”. We then convert the result into a numeric column using as.numeric:\n\ncrsp &lt;- crsp |&gt; \n  mutate(\n    ret = as.numeric(if_else(ret %in% c(\"B\", \"C\"), NA, ret)),\n    dlret = as.numeric(if_else(dlret %in% c(\"A\", \"S\", \"T\", \"P\"), NA, dlret)),\n  )\n\nNext, we need to adjust the monthly returns (ret) column. When a stock gets delisted, the delisting return is not fully incorporated in that return variable. We thus need to adjust. We do so by adding a delisting return whenever it is available (is.na(dlret) == FALSE) and the monthly return is not (is.na(ret) == TRUE). This is a bit of a quick-and-dirty way of adjusting, but suffices for our purposes.\n\ncrsp &lt;- crsp |&gt; \n  mutate(\n    ret = if_else(is.na(ret) == TRUE & is.na(dlret) == FALSE, dlret, ret)\n  ) |&gt; \n  select(-dlret, -dlstcd)"
  },
  {
    "objectID": "transform.html#generating-buy-and-hold-returns",
    "href": "transform.html#generating-buy-and-hold-returns",
    "title": "Part 2",
    "section": "Generating buy-and-hold-returns",
    "text": "Generating buy-and-hold-returns\nWith proper numeric monthly return date, we can now proceed to generate a yearly return measure that we can use with our yearly financial statement data. Remember, the return data is monthly data. So, our unit of observation in this data set is at the security-month level. In our compustat dataset, the unit of observation is the firm-fiscal year. We ultimately want to compare yearly changes in a firm profitability with a firm’s stock return over the same period. So we need a “yearly” return. We are going to compute that now. We are going to compute an excess buy-and-hold return:\n\\[EBHR_{i,t}^h = \\prod_{k=1}^h (1+Ret_{i,t+k}) - \\prod_{k=1}^h (1+MktRet_{t+k})\\] This is the return from buying a stock at the end of \\(t\\) and holding it for \\(h\\) months minus the return from a benchmark portfolio (usually some kind of market portfolio or similar).\nTo compute this variable, we need to compute leads and lags of our ret variable–a “lag” of 1 meaning the return of the month before, a lead of 1 meaning the next month’s return and so on. We will use collapse::flagfor this purpose. The following code creates lag 1 and 2 and lead 1 and 2 for the returns column and returns everything in a new matrix (a minus number–as in -2–stands for leads instead of lags):\n\nhead(flag(crsp$ret, n = -2:2), 24)\n\n             F2        F1        --        L1        L2\n [1,]  0.365385 -0.257143        NA        NA        NA\n [2,] -0.098592  0.365385 -0.257143        NA        NA\n [3,] -0.222656 -0.098592  0.365385 -0.257143        NA\n [4,] -0.005025 -0.222656 -0.098592  0.365385 -0.257143\n [5,] -0.080808 -0.005025 -0.222656 -0.098592  0.365385\n [6,] -0.615385 -0.080808 -0.005025 -0.222656 -0.098592\n [7,] -0.057143 -0.615385 -0.080808 -0.005025 -0.222656\n [8,] -0.242424 -0.057143 -0.615385 -0.080808 -0.005025\n [9,]  0.060000 -0.242424 -0.057143 -0.615385 -0.080808\n[10,] -0.377358  0.060000 -0.242424 -0.057143 -0.615385\n[11,] -0.212121 -0.377358  0.060000 -0.242424 -0.057143\n[12,]  0.000000 -0.212121 -0.377358  0.060000 -0.242424\n[13,] -0.384615  0.000000 -0.212121 -0.377358  0.060000\n[14,] -0.062500 -0.384615  0.000000 -0.212121 -0.377358\n[15,] -0.066667 -0.062500 -0.384615  0.000000 -0.212121\n[16,]  0.000000 -0.066667 -0.062500 -0.384615  0.000000\n[17,]        NA  0.000000 -0.066667 -0.062500 -0.384615\n[18,]  0.020408        NA  0.000000 -0.066667 -0.062500\n[19,]  0.025200  0.020408        NA  0.000000 -0.066667\n[20,]  0.009901  0.025200  0.020408        NA  0.000000\n[21,] -0.009804  0.009901  0.025200  0.020408        NA\n[22,] -0.013069 -0.009804  0.009901  0.025200  0.020408\n[23,] -0.010204 -0.013069 -0.009804  0.009901  0.025200\n[24,]  0.072165 -0.010204 -0.013069 -0.009804  0.009901\n\n\nWhere F stands for “Forward” and L stands for “Lag”.\nWe cannot use the function as is however. There are two things to consider with panel data when computing leads and lags. The first is that one has to be careful not to “lag into” the previous firm. Look at the following excerpt of the data to see the issue. The lag_ret column pushes a return from permno 10001 (from 2017) into the next security 10002.\n\ncrsp |&gt; \n  select(permno, date, ret) |&gt; \n  mutate(lag_ret = flag(ret, 1)) |&gt; \n  slice(395:405)\n\n# A tibble: 11 × 4\n   permno date            ret  lag_ret\n    &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001 2017-05-31  0.016   -0.0157 \n 2  10001 2017-06-30  0.0236   0.016  \n 3  10001 2017-07-31  0.00193  0.0236 \n 4  10001 2017-08-31  0.0116   0.00193\n 5  10002 1986-01-31 NA        0.0116 \n 6  10002 1986-02-28  0.140   NA      \n 7  10002 1986-03-31  0.0708   0.140  \n 8  10002 1986-04-30  0.0529   0.0708 \n 9  10002 1986-05-30 -0.0209   0.0529 \n10  10002 1986-06-30 -0.132   -0.0209 \n11  10002 1986-07-31  0.0493  -0.132  \n\n\nThe second issue is if that there might be implicit missing months in the data. For example if one row is February 1987 and the next row is April 1987 instead of March.\nBoth of these issues can be addressed by making the lagging function aware of the panel structure. It is not by default. To do so, we first create a year-month column\n\ncrsp$yrmon &lt;- as_yearmon(crsp$date)\nhead(crsp$yrmon)\n\n[1] \"Jan 1986\" \"Feb 1986\" \"Mar 1986\" \"Apr 1986\" \"May 1986\" \"Jun 1986\"\n\n\nNext we use collapse::findex_by to make functions like flag aware of the panel structure (permno-yearmonth).\n\ncrsp &lt;- crsp |&gt; \n1  mutate(yrmon = as_yearmon(date)) |&gt;\n2  findex_by(permno, yrmon) |&gt;\n  mutate(\n3    log_ret = log(ret + 1),\n4    BH12M_log_ret = rowSums(flag(log_ret, n = -3:8)),\n5    BHR12M = exp(BH12M_log_ret) - 1,\n6    log_mret = log(vwretd + 1),\n    BH12M_log_mret = rowSums(flag(log_mret, n = -3:8)),\n    BH12M_mret = exp(BH12M_log_mret) - 1,\n7    EBHR12M = BHR12M - BH12M_mret\n  ) |&gt; \n8  unindex() |&gt;\n  select(-log_ret, -log_mret, -BH12M_log_ret, -BH12M_log_mret, BH12M_mret)\n\n\n1\n\nCreate a {zoo} year-month variable\n\n2\n\nCreate panel indices. permno as the unit index and yrmon as the time index. Useful for functions such as flag to use\n\n3\n\nCreate a log return\n\n4\n\nKey part: flag creates a matrix of leads and lags from 8 months before the current months to 3 months after the current month (e.g, if we are in December from April of the current year to March of the next year). Then rowSums sums up all the values in a given row of the matrix to one value. This is a log buy-and-hold return.\n\n5\n\nExponentiate to turn the log BHR return into a normal gross return and subtract one to get make it a net return\n\n6\n\nRepeat the buy-and-hold return computation for the value-weighted market return\n\n7\n\nCreate the excess buy-and-hold return EBHR by subtracting the market BHR from the stock’s BHR\n\n8\n\nRemove the panel index again, as we do not need it anymore\n\n\n\n\nLet’s look at the differences in raw returns and buy-and-hold returns, just so that you can get a feeling for the differences in magnitude and the variation.\n\ncrsp |&gt; \n  select(ret, BHR12M, EBHR12M) |&gt; \n  descr()\n\nDataset: EBHR12M, 3 Variables, N = 2674972\n--------------------------------------------------------------------------------\nret (numeric): \nStatistics (1% NAs)\n         N   Ndist  Mean    SD  Min  Max  Skew    Kurt\n  2'648249  409862  0.01  0.19   -1   24  7.83  408.86\nQuantiles\n     1%     5%    10%    25%  50%   75%   90%   95%  99%\n  -0.42  -0.24  -0.16  -0.07    0  0.07  0.18  0.27  0.6\n--------------------------------------------------------------------------------\nBHR12M (numeric): \nStatistics (10.98% NAs)\n         N     Ndist  Mean    SD  Min     Max  Skew    Kurt\n  2'381141  2'364174  0.15  0.83   -1  106.54    14  816.75\nQuantiles\n     1%     5%    10%    25%   50%   75%   90%   95%   99%\n  -0.87  -0.67  -0.52  -0.24  0.05  0.35  0.78  1.21  2.87\n--------------------------------------------------------------------------------\nEBHR12M (numeric): \nStatistics (10.98% NAs)\n         N     Ndist  Mean    SD    Min     Max   Skew    Kurt\n  2'381141  2'380644  0.02  0.81  -1.63  105.92  14.99  908.41\nQuantiles\n     1%     5%    10%    25%    50%   75%  90%   95%   99%\n  -0.99  -0.75  -0.61  -0.34  -0.07  0.21  0.6  1.02  2.64\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "transform.html#merging-compustat-and-crsp-data",
    "href": "transform.html#merging-compustat-and-crsp-data",
    "title": "Part 2",
    "section": "Merging Compustat and CRSP data",
    "text": "Merging Compustat and CRSP data\nWe now get to another important and common task. Merging–or joining–data from different sources. In our case, we need to join the our financial data from Computstat to the 12-month buy and hold returns we just computed. For that purpose, we quickly create a data frame that contains only the return data we want to merge to financial data.\n\ncrsp_join &lt;- crsp |&gt; \n  select(permno, yrmon, BHR12M, EBHR12M) |&gt; \n  drop_na()\n\nWe join via what is called a left join. Left joins keep all rows from the “left” table/data frame and join all rows from the “right” table/data frame for which it finds matching keys. In our case the keys are the permno security identifier to identify the firm, and the year month of the fiscal year end. All rows in the left table for which we do not find a match in the right table are filled with missings for the columns BHR12M and EBHR12M. This is the syntax.\n\nsmple_raw &lt;- ccm |&gt; \n1  mutate(yrmon = as_yearmon(datadate)) |&gt;\n2  left_join(crsp_join, join_by(permno, yrmon))\n\n\n1\n\nCreate an corresponding {zoo} year-month variable from the fiscal year-end date column. Need this to join the two tables.\n\n2\n\nLeft-join the resulting data frame to crsp_join by the keys permno and yrmon (two columns with identical names that need to be present in both tables/data frames)\n\n\n\n\nLet us check how many matches we had using collapse::fnobs, which shows the number of non-missing observations per column\n\nfnobs(smple_raw)\n\n   gvkey linkprim     liid linktype   permno datadate    fyear      tic \n  239192   239192   239192   239192   239192   239192   239191   239192 \n   cusip     conm    curcd      fyr       at      ceq      che      dlc \n  239192   239192   239191   239191   218211   218050   217634   217255 \n    dltt       ib    oiadp     sale    exchg   costat      fic   priusa \n  217206   217787   217229   217266   239192   239192   239192   239192 \n     sic    yrmon   BHR12M  EBHR12M \n  239192   239192   187990   187990 \n\n\nWe had 239,192 observations in the compustat file and found a matching permno x yrmon combination 187,990 cases in the crsp_join file.\nWe are not dropping missing values just yet. We first want to compute the other variables.\nWe also want to double-check that we still have unique firm-fiscal-year combinations after the merge.\n\nsmple_raw |&gt; count(gvkey, fyear) |&gt; count(n)\n\nStoring counts in `nn`, as `n` already present in input\nℹ Use `name = \"new_name\"` to pick a new name.\n\n\n# A tibble: 1 × 2\n      n     nn\n  &lt;int&gt;  &lt;int&gt;\n1     1 239192\n\n\nLooks good. We only have gvkey x fyear combinations that occur exactly once."
  },
  {
    "objectID": "transform.html#generating-changes-in-return-on-capital-employed",
    "href": "transform.html#generating-changes-in-return-on-capital-employed",
    "title": "Part 2",
    "section": "Generating changes in return on capital employed",
    "text": "Generating changes in return on capital employed\nThe second to last step in getting the data ready for our analysis is creating the change in return on capital employed (ROCE). For simplicity, we define capital employed (CE) as equity (ceq) plus debt (dltt + dlc) minus cash and cash equivalents (che). We use operating income after depreciation (oiadp) as the numerator. Because we are dealing with a panel we have to use findex_by again to make the flag function aware of the panel structure\n\nsmple_raw &lt;- \n  smple_raw |&gt; \n1  drop_na(fyear) |&gt;\n  arrange(gvkey, fyear) |&gt; \n2  findex_by(gvkey, fyear) |&gt;\n  mutate(\n3    dltt = replace_NA(dltt, 0),\n    dlc = replace_NA(dlc, 0),\n    che = replace_NA(che, 0),\n4    cap_employed = ceq + dltt + dlc - che,\n5    roce = oiadp / ((cap_employed + flag(cap_employed, 1))/2),\n6    ch_roce = roce - flag(roce, 1)\n  ) |&gt; \n7  unindex()\n\n\n1\n\nDrop all rows with a missing fiscal year value\n\n2\n\nCreate panel indices. gvkey as the unit index and fyear as the time index. Mainly useful for functions such as flag\n\n3\n\nSome variables have missings that are more likely to be zeros. (e.g., if debt is missing, it likely does not have debt). `replace_NA`` replaces NA values with a chosen value (0 in this case)\n\n4\n\nCompute capital employed as explained above\n\n5\n\nCompute ROCE using average capital employed over the fiscal year\n\n6\n\nCompute a simple change in ROCE. We chose not to scale the change\n\n7\n\nRemove the panel index again, as we do not need it anymore"
  },
  {
    "objectID": "transform.html#final-filtering",
    "href": "transform.html#final-filtering",
    "title": "Part 2",
    "section": "Final filtering",
    "text": "Final filtering\nThe last step is to clean the data a bit more. We could have done this before, but it is usually best to do this in the end when all variables are computed (because of the panel structure, you do not wanna delete rows in the middle of computations that might involve leads and lags). Small Compustat firms are often full of outliers (e.g., negative equity and huge debt.) We want to look at the relation for “reasonably normal” companies. So we filter on having a minimum size in terms of total assets and revenues as well as slightly positive equity. We then drop all missing values.\n\nsmple0 &lt;- smple_raw |&gt; \n  select(gvkey, fyear, conm, yrmon, roce, ch_roce, EBHR12M, sic, sale, at, ceq) |&gt; \n  filter(\n    at &gt; 50, \n    sale &gt; 50,\n    ceq &gt; 1\n  ) |&gt; \n  drop_na()\n\nLet us look at the distribution of our main variables of interest\n\nsmple0 |&gt; \n  select(ch_roce, EBHR12M) |&gt; \n  descr()\n\nDataset: EBHR12M, 2 Variables, N = 102823\n--------------------------------------------------------------------------------\nch_roce (numeric): Operating Income After Depreciation\nStatistics\n       N   Ndist   Mean     SD       Min      Max    Skew      Kurt\n  102823  102823  -0.04  22.21  -4591.53  1913.35  -84.58  19881.56\nQuantiles\n     1%     5%    10%    25%  50%   75%   90%   95%   99%\n  -2.09  -0.31  -0.16  -0.05   -0  0.03  0.12  0.24  2.04\n--------------------------------------------------------------------------------\nEBHR12M (numeric): \nStatistics\n       N   Ndist  Mean    SD    Min    Max  Skew    Kurt\n  102823  102823  0.05  0.66  -1.45  34.84  9.82  281.43\nQuantiles\n     1%    5%    10%    25%    50%   75%   90%   95%   99%\n  -0.84  -0.6  -0.47  -0.25  -0.03  0.21  0.55  0.89  2.26\n--------------------------------------------------------------------------------\n\n\nWe can still see huge outliers. You can explore the reasons for those at your own leasure. You will find things like firm-years with negative capital employed heavily distorting the ROCE and similar things. We could (and maybe should) spend more time cleaning those cases. An alternative you often see is to just trim outliers, assuming they are all irregular patterns. We will do this now too for expediency. We will remove the bottom and top 1% of change in ROCE observations:\n\nsmple &lt;- smple0 |&gt; \n  filter(\n    ch_roce &lt; quantile(ch_roce, 0.99), \n    ch_roce &gt; quantile(ch_roce, 0.01)\n  )\n\nWith returns it is more likely that large outliers are features of the data and not irregular outliers. BHR returns can be very heavily be influenced by a few large values however. Still, we don’t filter the BHR returns for now."
  },
  {
    "objectID": "transform.html#visualizing-the-bhr-roce-relation",
    "href": "transform.html#visualizing-the-bhr-roce-relation",
    "title": "Part 2",
    "section": "Visualizing the BHR-ROCE relation",
    "text": "Visualizing the BHR-ROCE relation\nTo motivate the plot we are about to make, we will start with a useless plot:\n\nsmple |&gt; \n  ggplot(aes(x = ch_roce, y = EBHR12M)) + \n  geom_point(size = 0.5, shape = 21)\n\n\n\n\n\n\n\n\nThis is a prime example of over-plotting. Too many points on top of each other. Even if we would make them partially transparent, there is no chance you will see a pattern between changes in ROCE and excess buy-and-hold returns in this mess. You cannot even see where most of the points are.\nOur workaround to uncovering structure is twofold. First we color the points according to where most of the points are starting from the center. Second, we draw a flexible trendline through the point cloud.\n\n1fig2 &lt;- smple |&gt;\n2  ggplot(aes(x = ch_roce, y = EBHR12M)) +\n3  ggdensity::geom_hdr_points(size = 0.5, shape = 21) +\n4  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n5  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n6  geom_smooth(\n    method = 'gam',\n    formula = y ~ s(x, bs = \"cs\"),\n    color = \"tan2\",\n    linewidth = 1.5,\n    se = FALSE\n    )+\n7  coord_cartesian(xlim = c(-1, +1), ylim = c(-1, +1)) +\n8  theme_light() +\n9  theme(panel.grid.minor = element_blank()) +\n10  labs(\n    y = \"Contemporaneous excess buy-and-hold return\",\n    x = \"Yearly change in return on capital employed\",\n    title = \"The relation between profitability changes and returns is S-shaped\",\n    caption = \"Source: Compustat North America and CRSP monthly stock data (1980 - 2022)\"\n  )\nfig2\n\n\n1\n\nStore the result of the computation into variable called fig2\n\n2\n\nCreate a plotting canvas with the x axis being mapped to ch_roce and the y axis to EBHR12M\n\n3\n\nDraw a point cloud colored by high density regions (hdrs). The size argument controls the size of the points, and shape = 21 makes them open points\n\n4\n\nAdd a guiding vertical and a horizontal line that intersects at 0. To make the symmetry of S-shape even more obvious\n\n5\n\nReduce the padding around the x and y axes to zero. This way it becomes a bit more obvious that we “zoom” into the data\n\n6\n\ngeom_smooth generates a smooth trendline. We need to specify a method for drawing this and we choose GAM (general additive model) with a cubic spline formula.\n\n7\n\ncoord_cartesian allows us to “zoom” into the data without throwing away points. This is important as we want geom_smooth to draw the trendline taking into account all points, not only the ones we see into the zoomed-in plot\n\n8\n\nSwitch the standard grey theme to a lighter theme\n\n9\n\nRemove some unnecessary gridlines\n\n10\n\nAdd annotations to finalize the plot\n\n\n\n\n\n\n\n\n\n\n\nJust to make sure our results are not sensitive to extreme returns we can re-draw the graph after filtering out extreme EBHR12M values:\n\nsmple |&gt; \n  filter(\n    EBHR12M &lt; quantile(EBHR12M, 0.99),\n    EBHR12M &gt; quantile(EBHR12M, 0.01)\n  ) |&gt; \n  ggplot(aes(x = ch_roce, y = EBHR12M)) + \n  ggdensity::geom_hdr_points(size = 0.5, shape = 21) +\n  geom_vline(xintercept = 0) + \n  geom_hline(yintercept = 0) + \n  scale_x_continuous(expand = c(0, 0)) + \n  scale_y_continuous(expand = c(0, 0)) + \n  geom_smooth(\n    method = 'gam', \n    formula = y ~ s(x, bs = \"cs\"),\n    color = \"tan2\",\n    linewidth = 1.5,\n    se = FALSE\n    )+ \n  coord_cartesian(xlim = c(-1, +1), ylim = c(-1, +1)) +\n  theme_light() +\n  theme(panel.grid.minor = element_blank()) + \n  labs(\n    y = \"Contemporaneous excess buy-and-hold return\",\n    x = \"Yearly change in return on capital employed\",\n    title = \"The relation between profitability changes and returns is S-shaped\",\n    caption = \"Source: Compustat North America and CRSP monthly stock data (1980 - 2022)\\nExtreme returns were removed from this figure.\"\n  )\n\n\n\n\n\n\n\n\nThe S-shape is still there. So let’s save the final result of our efforts. The graph clearly shows a non-linear association between yearly changes in firm profitability and buy-and-hold returns over the same period as the change. This can have many implications of course. It might be that larger changes are less “persistent”, investors expect them to reverse. It might be that larger changes are more likely to be anticipated. Etc.\n\nggsave(\"chroce-bhrets.png\", fig2, width = 6, height = 5, units = \"in\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site contains the material for the introductory R workshop, offered for Tilburg Master of Accountancy students. The workshop is a short introduction, consisting of two sessions (1h:45min each). Its purpose is to introduce Master students with no previous programming experience to the R programming language—especially with an eye towards the Master file and courses, such as Data-Driven Decision-Making. But we hope everyone with an interest in learning R will find something useful here."
  },
  {
    "objectID": "index.html#what-is-this-site",
    "href": "index.html#what-is-this-site",
    "title": "Welcome",
    "section": "",
    "text": "This site contains the material for the introductory R workshop, offered for Tilburg Master of Accountancy students. The workshop is a short introduction, consisting of two sessions (1h:45min each). Its purpose is to introduce Master students with no previous programming experience to the R programming language—especially with an eye towards the Master file and courses, such as Data-Driven Decision-Making. But we hope everyone with an interest in learning R will find something useful here."
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Welcome",
    "section": "Learning goals",
    "text": "Learning goals\n\nBe able to perform basic data operations in R using the tidyverse dialect\nBe able to construct standard data visualizations using the {ggplot2} R package\nBe able to compute standard descriptive statistics"
  },
  {
    "objectID": "index.html#how-to-use-this-site",
    "href": "index.html#how-to-use-this-site",
    "title": "Welcome",
    "section": "How to use this site",
    "text": "How to use this site\nYou only learn coding by doing it. The site complements but is not a substitute for the workshop’s in-class learning. It is written with users in mind that have been at the workshop. It provides a summary of what we code there plus the rationale discussed. We will use the website margins for further commentary on coding rationale.  We hope the site will help you to refresh your memory after the workshop when you start using R in your studies.Margin comments, such as this one, contain comments on coding practices"
  },
  {
    "objectID": "index.html#other-useful-ressources",
    "href": "index.html#other-useful-ressources",
    "title": "Welcome",
    "section": "Other useful ressources",
    "text": "Other useful ressources\nThis is a short, condensed workshop. We only have three and a half hours to introduce you to the basics, so we need to be quite selective. Here are a number of great resources to help you build on what you learned.\n\nR for Data Science. This is a complete, open book introducing the tidyverse dialect of R for data science. It goes into more detail and extends on the basic data wrangling techniques that we covered. A great place if you look for more coverage of the core data manipulation methods.\nEmpirical Research in Accounting: Tools and Methods. A great ressource maintained by Ian Gow and Tony Ding. It contains R code and is essentially the companion to a course on financial accounting research that begins at an upper-undergraduate (“honours”) or introductory PhD level.\nData-Driven Decision-Making Lecture Notes. The lecture notes of the controlling track course “Data-Driven Decision-Making” contains further examples, cases and material on using R for data-driven decision making. For example, one chapter of interest might be the chapter on predictive analytics, introducing R frameworks to coding prediction models.\nTilburg Science Hub. You’ll find many useful tutorials here. The TSH is a great knowledge repository dedicated to help young researchers developing their empirical skill set. It offers many tutorials on how to process data, design workflows, and structure projects.\n\nBest regards\nHarm Schütt Office: Koopmans Buidling, K 250"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "The Tilburg master of Accountancy is a one-year program, structured to teach you how to design and assess the financial and non-financial information environment of companies. In addition, you get solid foundation in emerging themes such as incorporating and reporting of sustainability strategies and extracting valuable information from accounting data by using state-of-the-art data analytics.\nYou can find more information here (in Dutch) and a general overview of the program here (in English)"
  },
  {
    "objectID": "about.html#tilburg-master-of-accountancy",
    "href": "about.html#tilburg-master-of-accountancy",
    "title": "About us",
    "section": "",
    "text": "The Tilburg master of Accountancy is a one-year program, structured to teach you how to design and assess the financial and non-financial information environment of companies. In addition, you get solid foundation in emerging themes such as incorporating and reporting of sustainability strategies and extracting valuable information from accounting data by using state-of-the-art data analytics.\nYou can find more information here (in Dutch) and a general overview of the program here (in English)"
  },
  {
    "objectID": "about.html#the-instructor",
    "href": "about.html#the-instructor",
    "title": "About us",
    "section": "The Instructor",
    "text": "The Instructor\nThis course is taught by Harm Schütt, Assistant professor at the Tilburg School of Economics and Management. My personal website provides more information on my research and teaching activities."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Part 1",
    "section": "",
    "text": "This part discusses a simple example of loading and examining financial data from WRDS. We discuss how to use coding to explore, understand, and document the characteristics of your data. While doing so you will learn the basics tidyverse and collapse commands. We will finish with a rudimentary analysis of changes in firm size over time by creating the following plot:"
  },
  {
    "objectID": "data.html#summary",
    "href": "data.html#summary",
    "title": "Part 1",
    "section": "",
    "text": "This part discusses a simple example of loading and examining financial data from WRDS. We discuss how to use coding to explore, understand, and document the characteristics of your data. While doing so you will learn the basics tidyverse and collapse commands. We will finish with a rudimentary analysis of changes in firm size over time by creating the following plot:"
  },
  {
    "objectID": "data.html#start-load-the-necessary-packages",
    "href": "data.html#start-load-the-necessary-packages",
    "title": "Part 1",
    "section": "Start: Load the necessary packages",
    "text": "Start: Load the necessary packages\nOnce R, RStudio, and the relevant R packages are installed we can start analyzing data. To load the data and start examining it, we want access to a few additional functions that are not in base R. The functions used in this chapter are contained in package collection called tidyverse. We also want to use some extra functionality in the {collapse} package. These packages contain very useful and popular extra functionality not included in base R. Anytime you want to load an R package for the extra functionality it brings, you type and execute a library function call:\nPut your library statements, etc. at the top of each script. This makes it easier to see what packages are needed to run the code that follows\n\nlibrary(collapse)\n\ncollapse 1.9.6, see ?`collapse-package` or ?`collapse-documentation`\n\n\n\nAttache Paket: 'collapse'\n\n\nDas folgende Objekt ist maskiert 'package:stats':\n\n    D\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()      masks stats::filter()\n✖ lubridate::is.Date() masks collapse::is.Date()\n✖ dplyr::lag()         masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThese two lines load the collapse package and the core tidyverse packages like ggplot2, dplyr, etc. It tells you in the message under “Attaching core tidyverse packages” exactly which packages it attached and which versions. It also tells you which functions from the loaded packages have a name conflict with functions in base R (or from other packages you might have loaded).\n\n\nIf you get an error there is no package called ‘tidyverse’, you’ll need to first install it, then run library() again.The command is\ninstall.packages(\"tidyverse\")\nYou only need to install a package once, but you need to load it every time you start a new session.\nOnce we have the packages loaded, we can start using their extra functionality. The tidyverse packages introduce an R coding framework that is slightly different from base R. Arguably, it is easier and still powerful. These days, it is probably the most common way of coding in R, which is why we chose to teach you using tidyverse mechanics and not pure base R. This does not mean that you cannot do what we do with standard R functions, but it is often clumsier."
  },
  {
    "objectID": "data.html#getting-data",
    "href": "data.html#getting-data",
    "title": "Part 1",
    "section": "Getting data",
    "text": "Getting data\nBecause you will mostly encounter financial data in your professional life as well as the master program, our examples will be based on standard financial databases. This should help you a lot later on, for example when you start your replication study. Normally you would download this data from WRDS or access the data directly from a database. In the interest of time, we provide you with a raw dataset."
  },
  {
    "objectID": "data.html#loading-data",
    "href": "data.html#loading-data",
    "title": "Part 1",
    "section": "Loading data",
    "text": "Loading data\nSay we have downloaded some data from the Compustat North America file from WRDS and put it into a folder called data/. We labelled the file “ccm-raw-2023-08-08-csv”. That name is deliberate as it denotes that the file contains the raw, untouched data and shows the date it was downloaded.\nThe Compustat North America file contains some selected financial variables for publicly listed firms in North America. We want to load that file into R, so that we can examine it. The following code line does that:\n\nccm_raw &lt;- read_csv(\"data/ccm-raw-2023-08-08-csv.zip\")\n\nRows: 271143 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): GVKEY, LINKPRIM, LIID, LINKTYPE, indfmt, consol, popsrc, datafmt,...\ndbl  (12): LPERMNO, fyear, fyr, at, ceq, che, dlc, dltt, ib, oiadp, sale, exchg\ndate  (1): datadate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nreadr::read_csv is a function for—your guessed it—loading “.csv” files (csv: comma separated value).1  This line can be read from left to right as: “create an object called ccm_raw, assign (&lt;-) to it the result of the function call read_csv()”. The result of read_csv is whatever that function does to its input. The input in this case is a file path in string form: “data/ccm-raw-2023-08-08-csv.zip”. Good function names tell you what the function does. In this case, it is pretty expressive: the function reads in a csv file. (Here, it actually reads in a zip file that contains a .csv file.) So what is the content of ccm_raw now? If you type the object name into the console and hit execute, you see that ccm_raw is a tibble, a type of data table.When we first introduce a function, we will use a package::function notation (e.g., readr::read_csv) to point to the package the function is included in. So that you know what packages need to be loaded before you can run the code\n\nccm_raw\n\n# A tibble: 271,143 × 29\n   GVKEY  LINKPRIM LIID  LINKTYPE LPERMNO datadate   fyear indfmt consol popsrc\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n 1 001001 P        01    LU         10015 1983-12-31  1983 INDL   C      D     \n 2 001001 P        01    LU         10015 1984-12-31  1984 INDL   C      D     \n 3 001001 P        01    LU         10015 1985-12-31  1985 INDL   C      D     \n 4 001003 C        01    LU         10031 1983-12-31  1983 INDL   C      D     \n 5 001003 C        01    LU         10031 1984-12-31  1984 INDL   C      D     \n 6 001003 C        01    LU         10031 1986-01-31  1985 INDL   C      D     \n 7 001003 C        01    LU         10031 1987-01-31  1986 INDL   C      D     \n 8 001003 C        01    LU         10031 1988-01-31  1987 INDL   C      D     \n 9 001003 C        01    LU         10031 1989-01-31  1988 INDL   C      D     \n10 001004 P        01    LU         54594 1981-05-31  1980 INDL   C      D     \n# ℹ 271,133 more rows\n# ℹ 19 more variables: datafmt &lt;chr&gt;, tic &lt;chr&gt;, cusip &lt;chr&gt;, conm &lt;chr&gt;,\n#   curcd &lt;chr&gt;, fyr &lt;dbl&gt;, at &lt;dbl&gt;, ceq &lt;dbl&gt;, che &lt;dbl&gt;, dlc &lt;dbl&gt;,\n#   dltt &lt;dbl&gt;, ib &lt;dbl&gt;, oiadp &lt;dbl&gt;, sale &lt;dbl&gt;, exchg &lt;dbl&gt;, costat &lt;chr&gt;,\n#   fic &lt;chr&gt;, priusa &lt;chr&gt;, sic &lt;chr&gt;\n\n\nTibbles are the tidyverse versions of data tables. Similar to tables you are familiar with from Excel. They are also called data frames in many statistical programming languages. Data frames are the data container type you will encounter most. There are other container types, like matrix, which is a simpler data container that is mostly used for matrix algebra. We won’t spend time on those, because you’ll be mostly concerned with data in table form.2\n\nccm_raw &lt;- rename(ccm_raw, \n                  gvkey = GVKEY, permno = LPERMNO,\n                  liid = LIID, linktype = LINKTYPE, linkprim = LINKPRIM)\n\n\nAdding labels to columns\nThis is not mandatory, but can make your life a lot easier. Especially if you get data with semi-cryptic names. Assigning labels to to columns will help you (and potential coauthors) later to remember what you are looking at. Some functions can also take advantage of labels. collapse::vlabels can be used to assign labels. You do so by assigning a “named vector” (using the concatenate c() function to create the vector) to vlabels(DATAFRAMENAME):You can also just rename columns; give them more expressive names. For commercial datasets, we think it is better to keep the standard names for the raw data.\n\nvlabels(ccm_raw) &lt;- c( \n  # order must be the same as the columns:\n  \"gvkey\" = \"Standard and Poor's Identifier\",\n  \"linkprim\" = \"Primary Link Marker\",\n  \"liid\" = \"Security-level Identifier\",\n  \"linktype\" = \"Link Type Code\",\n  \"permno\" = \"Historical CRSP Identifier\",\n  \"datadate\" = \"Fiscal Year-End Date\",\n  \"fyear\" = \"Data Year - Fiscal\",\n  \"indfmt\" = \"Industry Format\",\n  \"consol\" = \"Consolidation Level\",\n  \"popsrc\" = \"Population Source\",\n  \"datafmt\" = \"Data Format\",\n  \"tic\" = \"Ticker Symbol\",\n  \"cusip\" = \"CUSIP\",\n  \"conm\" = \"Company Name\",\n  \"curcd\" = \"Currency\",\n  \"fyr\" = \"Fiscal Year-End\",\n  \"at\" = \"Assets - Total\",\n  \"ceq\" = \"Common/Ordinary Equity - Total\",\n  \"che\" = \"Cash and Short-Term Investments\",\n  \"dlc\" = \"Debt in Current Liabilities - Total\",\n  \"dltt\" = \"Long-Term Debt - Total\",\n  \"ib\" = \"Income Before Extraordinary Items\",\n  \"oiadp\" = \"Operating Income After Depreciation\",\n  \"sale\" = \"Sales/Turnover (Net)\",\n  \"exchg\" = \"Stock Exchange Code\",\n  \"costat\" = \"Company Status\",\n  \"fic\" = \"Foreign Incorporation Code\",\n  \"priusa\" = \"PRIUSA -- Current Primary Issue Tag\",\n  \"sic\" = \"Standard Industry Classification Code\"\n)"
  },
  {
    "objectID": "data.html#examining-data",
    "href": "data.html#examining-data",
    "title": "Part 1",
    "section": "Examining data",
    "text": "Examining data\n\nTable meta data functions\nWhen you typed ccm_raw, R did not print the full table. It tells you that there are 271,133 more rows and 19 more variables that you don’t see here. This printing behavior is mostly to save you from accidentally outputting huge datasets and freeze your machine. There are other functions to help you get a better overview of the data contained in the tibble. One is dplyr::glimpse\n\nglimpse(ccm_raw)\n\nRows: 271,143\nColumns: 29\n$ gvkey    &lt;chr&gt; \"001001\", \"001001\", \"001001\", \"001003\", \"001003\", \"001003\", \"…\n$ linkprim &lt;chr&gt; \"P\", \"P\", \"P\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"P\", \"P\", \"P\", \"…\n$ liid     &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n$ linktype &lt;chr&gt; \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"LU\", \"…\n$ permno   &lt;dbl&gt; 10015, 10015, 10015, 10031, 10031, 10031, 10031, 10031, 10031…\n$ datadate &lt;date&gt; 1983-12-31, 1984-12-31, 1985-12-31, 1983-12-31, 1984-12-31, …\n$ fyear    &lt;dbl&gt; 1983, 1984, 1985, 1983, 1984, 1985, 1986, 1987, 1988, 1980, 1…\n$ indfmt   &lt;chr&gt; \"INDL\", \"INDL\", \"INDL\", \"INDL\", \"INDL\", \"INDL\", \"INDL\", \"INDL…\n$ consol   &lt;chr&gt; \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"…\n$ popsrc   &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"…\n$ datafmt  &lt;chr&gt; \"STD\", \"STD\", \"STD\", \"STD\", \"STD\", \"STD\", \"STD\", \"STD\", \"STD\"…\n$ tic      &lt;chr&gt; \"AMFD.\", \"AMFD.\", \"AMFD.\", \"ANTQ\", \"ANTQ\", \"ANTQ\", \"ANTQ\", \"A…\n$ cusip    &lt;chr&gt; \"000165100\", \"000165100\", \"000165100\", \"000354100\", \"00035410…\n$ conm     &lt;chr&gt; \"A & M FOOD SERVICES INC\", \"A & M FOOD SERVICES INC\", \"A & M …\n$ curcd    &lt;chr&gt; \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\"…\n$ fyr      &lt;dbl&gt; 12, 12, 12, 12, 12, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ at       &lt;dbl&gt; 14.080, 16.267, 39.495, 8.529, 8.241, 13.990, 14.586, 16.042,…\n$ ceq      &lt;dbl&gt; 7.823, 8.962, 13.014, 6.095, 6.482, 6.665, 7.458, 7.643, -0.1…\n$ che      &lt;dbl&gt; 4.280, 1.986, 2.787, 2.023, 0.844, 0.005, 0.241, 0.475, 0.302…\n$ dlc      &lt;dbl&gt; 0.520, 0.597, 8.336, 0.250, 0.350, 0.018, 0.013, 0.030, 7.626…\n$ dltt     &lt;dbl&gt; 4.344, 4.181, 11.908, 0.950, 0.600, 4.682, 3.750, 5.478, 0.10…\n$ ib       &lt;dbl&gt; 1.135, 1.138, 2.576, 1.050, 0.387, 0.236, 0.793, -0.525, -7.8…\n$ oiadp    &lt;dbl&gt; 1.697, 1.892, 5.238, 2.089, 0.732, 0.658, 1.933, -0.405, -4.3…\n$ sale     &lt;dbl&gt; 25.395, 32.007, 53.798, 13.793, 13.829, 24.189, 36.308, 37.35…\n$ exchg    &lt;dbl&gt; 14, 14, 14, 19, 19, 19, 19, 19, 19, 11, 11, 11, 11, 11, 11, 1…\n$ costat   &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"A\", \"A\", \"A\", \"…\n$ fic      &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\"…\n$ priusa   &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n$ sic      &lt;chr&gt; \"5812\", \"5812\", \"5812\", \"5712\", \"5712\", \"5712\", \"5712\", \"5712…\n\n\nglimpse just shows you in compact form: 1) the number of rows, 2) the number of columns, 3) all column names, 4) column types, and 5) the first values in each column in the table\nAn alternative to glimpse that does not rely on dplyr being loaded in is the base R function: base::str. But it is usually less clean, so we prefer glimpse.\n\n\nData viewers\nIf you use RStudio or VScode as your IDE (integrated development environment), then you can also view tibbles/data.frames/data.tables in a viewer. In RStudio, you can click on the tibble name “ccm_raw” in the environment pane, hitting the F2-key if the cursor is on the “ccm_raw” also opens the data viewer. Or you can use the function tibble::view to start the data viewer.\n\nview(ccm_raw)\n\n\n\nGetting a structured overview\nWhen dealing with large datasets, looking at the data viewer will not be enough. One of the first things you should do is to get a better feeling for the data you are dealing with. If you want to get a glimpse of what countries are in this dataset, you cannot just browse through 300,000 rows. It will take too long and it is easy to miss a rarely occurring patterns.\nThings like examining the distinct values in a column help you a lot in better understanding your data. You can use the collapse::fndistinct function to check the number of distinct values per column\n\nfndistinct(ccm_raw)\n\n   gvkey linkprim     liid linktype   permno datadate    fyear   indfmt \n   25614        4       23        2    26099      518       44        1 \n  consol   popsrc  datafmt      tic    cusip     conm    curcd      fyr \n       1        1        1    25613    25614    25612        2       12 \n      at      ceq      che      dlc     dltt       ib    oiadp     sale \n  203359   176724   113664    70320   117375   118348   130310   185004 \n   exchg   costat      fic   priusa      sic \n      15        2       64       23      449 \n\n\nThere are a few columns with just one distinct value. We don’t need those anymore. They were used to filter the raw data. In case you do not know/remember what those columns were, look at the labels.\n\n1ccm_raw |&gt;\n2  select(indfmt, consol, popsrc, datafmt) |&gt;\n3  namlab()\n\n\n1\n\nStart with the ccm_raw dataset\n\n2\n\nSelect four columns, throw the rest away (using dplyr::select)\n\n3\n\nCall namlabon the reduced tibble to see the column labels for the selected labels\n\n\n\n\n  Variable               Label\n1   indfmt     Industry Format\n2   consol Consolidation Level\n3   popsrc   Population Source\n4  datafmt         Data Format\n\n\n\nccm_raw |&gt;                                    \n  select(indfmt, consol, popsrc, datafmt) |&gt;  \n1  funique()\n\n\n1\n\ncollapse::funique shows only unique combinations in a data frame.\n\n\n\n\n# A tibble: 1 × 4\n  indfmt consol popsrc datafmt\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  \n1 INDL   C      D      STD    \n\n\nAbove, we used what is called a pipe operator |&gt;. It allows us to chain together a sequence of steps. When you see a |&gt;, it means “take the result of what is left of |&gt; and put it as the first input to what is right of |&gt;.\nBack to the topic at hand, we do not need the columns: indfmt, consol, popsrc, datafmt. Let us get rid of them. We do so by generating a new dataset, which we call ccm and using a “negative” select call.We like to keep the raw data untouched when filtering and cleaning data\n\nccm &lt;- \n  ccm_raw |&gt; \n  select(-indfmt, -consol, -popsrc, -datafmt) \n\n\n\nExamining individual columns\nYou would usually spend some more time understanding the data. For example, what about those with 2 values? We can use collapse::descr to provide some descriptive statistics. Notice how it also uses the labels we provided before.\n\nccm |&gt; \n  select(curcd, costat) |&gt; \n  descr()\n\nDataset: costat, 2 Variables, N = 271143\n--------------------------------------------------------------------------------\ncurcd (character): Currency\nStatistics (0% NAs)\n       N  Ndist\n  271141      2\nTable\n       Freq   Perc\nUSD  266735  98.38\nCAD    4406   1.62\n--------------------------------------------------------------------------------\ncostat (character): Company Status\nStatistics\n       N  Ndist\n  271143      2\nTable\n     Freq   Perc\nI  162350  59.88\nA  108793  40.12\n--------------------------------------------------------------------------------\n\n\nDepending on how we define our setting, we might want to get rid of those financials in Canadian dollars curcd == \"CAD\". costatshows that 60% of our sample has the inactive status.\nWe can also call descr on single columns using $ to select a single column:\n\ndescr(ccm$fic)\n\nDataset: fic, 1 Variables, N = 271143\n--------------------------------------------------------------------------------\nfic (character): Foreign Incorporation Code\nStatistics\n       N  Ndist\n  271143     64\nTable\n                 Freq   Perc\nUSA            241894  89.21\nCAN              7703   2.84\nCYM              2781   1.03\nISR              2418   0.89\nGBR              2195   0.81\nBMU              1806   0.67\nIRL              1070   0.39\nJPN               973   0.36\nNLD               868   0.32\nVGB               783   0.29\nMHL               656   0.24\nMEX               634   0.23\nAUS               537   0.20\nFRA               512   0.19\n... 50 Others    6313   2.33\n\nSummary of Table Frequencies\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3      31     130    4237     373  241894 \n--------------------------------------------------------------------------------\n\n\nThere are 69 different countries of incorporation in this dataset. Most incorporated in the USA. We learn something interesting about the dataset. Not all firms in the North America Dataset are located in North America. Most firm-year obs are associated with USA incorporation. Some are incorporated in Canada, the next most often occurring is Cayman islands. Depending on our research question, we might want to restrict our sample only to firms incorporated in the US. Before making that call, however, we might want to explore the data a bit more.\nWe see that some observations are from firms incorporated in the Netherlands. Let us extract a sample of names of those companies.\n\n1ccm |&gt;\n2  filter(fic == \"NLD\") |&gt;\n3  pull(conm) |&gt;\n4  unique() |&gt;\n5  head(20)\n\n\n1\n\nStart with the table ccm_raw\n\n2\n\nReduce the table to only rows with fic equaling (==) “NLD”\n\n3\n\ndplyr::pull extracts the “conm” column from the reduce table into a vector\n\n4\n\nReduce to only unique values in the vector\n\n5\n\nShow the first 20 values in the vector\n\n\n\n\n [1] \"ASM INTERNATIONAL NV\"         \"KLM-ROYAL DUTCH AIRLINES\"    \n [3] \"OCE NV\"                       \"KONINKLIJKE PHILIPS NV\"      \n [5] \"ROYAL DUTCH PETROLEUM NV\"     \"AUSIMONT NV\"                 \n [7] \"FIAT CHRYSLER AUTOMOBILES NV\" \"AKZO NOBEL NV\"               \n [9] \"ABN-AMRO HOLDINGS NV\"         \"AEGON NV\"                    \n[11] \"ING GROEP NV\"                 \"FRANK'S INTL NV\"             \n[13] \"PROSENSA HOLDING NV\"          \"POLYGRAM NV\"                 \n[15] \"PATHEON NV\"                   \"MOBILEYE NV\"                 \n[17] \"CNOVA NV\"                     \"PROQR THERAPEUTICS NV\"       \n[19] \"AFFIMED NV\"                   \"MEMOREX TELEX NV  -ADR\"      \n\n\nThe NLD firms are likely larger internationals that are listed on one of the US exchanges in some form, so that they have to file with the SEC (Companies listed on a US exchange must file financial statements with the SEC). Let’s look at ASM for example:\n\n1ccm |&gt;\n2  filter(conm == \"ASM INTERNATIONAL NV\") |&gt;\n3  select(tic, cusip, conm, curcd, exchg) |&gt;\n4  distinct()\n\n\n1\n\nStart with the table ccm_raw\n\n2\n\nReduce the table to only rows with conm equaling (==) “ASM INTERNATIONAL NV”\n\n3\n\nReduce the reduced table further to only contain the columns tic, cusip, conm, curcd, exchg\n\n4\n\ndplyr::distinctdrops all repeating rows in the reduced table (could have also used funique)\n\n\n\n\n# A tibble: 1 × 5\n  tic   cusip     conm                 curcd exchg\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                &lt;chr&gt; &lt;dbl&gt;\n1 ASMIY N07045102 ASM INTERNATIONAL NV USD      19\n\n\nAs the example above shows: dplyr::select and dplyr::filter are two fundamental slicing and indexing functions. filter is for rows and selectis for columns.\nThe purpose here was to examine what ticker, currency, and stock exchange code ASM is associated with. We also want to check there is more than one combination of values for these five columns (there is not). For example, ASM is in our sample for more than one year and it might have changed its ticker over time, etc.). We see that ASM has a ticker and an exchg code of 19, which means “Other-OTC” per the compustat data guide. These can include international filers. In fact, most of the observations fall into category 19 (another one, 14, is NASDAQ). Depending on the intended sample—US firms or firms listed in the US—we might want to exclude them."
  },
  {
    "objectID": "data.html#ensure-one-observation-per-unit-of-analysis",
    "href": "data.html#ensure-one-observation-per-unit-of-analysis",
    "title": "Part 1",
    "section": "Ensure one observation per unit of analysis",
    "text": "Ensure one observation per unit of analysis\nWhen you perform analyses, it is important that you are clear what your unit of analysis is. Let’s say that our unit of analysis is a firm-fiscal year. In that case, we need to check whether we have one observation per unit of analysis.\nWe can do this we simple counts.dplyr::count is a handy function for this. count, as the name suggests, counts how often a value of a set of variables occurs in the dataset. The first input you provide is the dataset, the next inputs are the column names to base the count on:\n\nccm |&gt; \n1  count(gvkey, datadate) |&gt;\n2  head()\n\n\n1\n\nCount how many observations exist for each unique gvkey x datadate combination\n\n2\n\nShow the first six observations in the resulting tibble\n\n\n\n\n# A tibble: 6 × 3\n  gvkey  datadate       n\n  &lt;chr&gt;  &lt;date&gt;     &lt;int&gt;\n1 001001 1983-12-31     1\n2 001001 1984-12-31     1\n3 001001 1985-12-31     1\n4 001003 1983-12-31     1\n5 001003 1984-12-31     1\n6 001003 1986-01-31     1\n\n\nFor example, the combination gvkey 001001 x date 1983-12-31 occurs exactly once in the datset ccm. We can combine two count calls to see whether we have anything to worry about:\n\nccm |&gt; \n1  count(gvkey, datadate, name = \"firm_year\") |&gt;\n2  count(firm_year)\n\n\n1\n\nCount how many observations exist for each unique gvkey x datadate combination. Call the count column “firm_year”\n\n2\n\nCount how often each value of the “firm_year” column occurs\n\n\n\n\n# A tibble: 3 × 2\n  firm_year      n\n      &lt;int&gt;  &lt;int&gt;\n1         1 265776\n2         2   2607\n3         3     51\n\n\nAs you can see, there are a few gvkey x datadate combinations that occur twice, some even three times. Why is this happening? Let write some code to figure it out.\n\nccm_test &lt;- ccm |&gt; \n1  add_count(gvkey, datadate, name = \"n_same_firm_year\") |&gt;\n2  filter(n_same_firm_year &gt; 1)\n\n\n1\n\ndplyr::add_count counts the number of rows with a specific gvkey x datadate value and adds that value to the rows with the respective gvkey x datadate value\n\n2\n\nNext we filter to only keep rows with a count higher than 1\n\n\n\n\nLet’s look at the result and limit our output to only some selected columns and ten rows:\n\nccm_test |&gt; \n  select(gvkey, permno, datadate, liid, linkprim, linktype, exchg) |&gt; \n  head(10)\n\n# A tibble: 10 × 7\n   gvkey  permno datadate   liid  linkprim linktype exchg\n   &lt;chr&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n 1 001076  10517 1993-03-31 01    J        LC          11\n 2 001076  78049 1993-03-31 02    P        LC          11\n 3 001076  78049 1994-03-31 02    P        LC          11\n 4 001076  10517 1994-03-31 01    J        LC          11\n 5 001076  10517 1995-03-31 01    J        LC          11\n 6 001076  78049 1995-03-31 02    P        LC          11\n 7 001076  78049 1995-12-31 02    P        LC          11\n 8 001076  10517 1995-12-31 01    J        LC          11\n 9 001076  10517 1996-12-31 01    J        LC          11\n10 001076  78049 1996-12-31 02    P        LC          11\n\n\nFrom the output we can see that the reason is that this dataset has another company identifier (permno) merged to it already. This is the identifier we will need in the second part to merge stock return data to this sample. It can happen that one gvkey is associated with more than one permno. Sometimes there are primary and secondary links.\nWe decided that our unit of analysis is firm-year, not security-year. Hence we only want one unique observation per firm-year. For that, we will need to do some filtering. Let us reduce the data to rows that either are a unique gvkey x datadate combination, or are labelled as primary link between compustat and crsp (which we will need later to join return data on it). We will also filter to companies incorporated in the US. (e.g., as we would want to if we look at some US-GAAP specific analysis).\n\nccm_unique &lt;- ccm |&gt; \n1  add_count(gvkey, datadate, name = \"n_same_firm_year\") |&gt;\n  filter(             \n2    n_same_firm_year == 1 | linkprim == \"P\",  # \"|\" means \"or\"\n3    fic %in% c(\"USA\")\n  ) |&gt;   \n4  select(-n_same_firm_year)\n\n\n1\n\nCount how many observations exist for each unique gvkey x datadate combination. Call the count column “firm_year”\n\n2\n\nKeep rows with either n_same_firm_year == 1 or linkprim == “P”\n\n3\n\nKeep rows with fic in a list of values (“USA”)\n\n4\n\nDelete the column “n_same_firm_year”\n\n\n\n\nWe should now have one observation per gvkey x datadate unit of analysis. Let us double-check whether it is indeed the case.\n\nccm_unique |&gt; \n  count(gvkey, datadate, name = \"firm_year\") |&gt; \n  count(firm_year)\n\n# A tibble: 1 × 2\n  firm_year      n\n      &lt;int&gt;  &lt;int&gt;\n1         1 239192\n\n\nWe will leave it at that for now and save the result."
  },
  {
    "objectID": "data.html#saving-datasets",
    "href": "data.html#saving-datasets",
    "title": "Part 1",
    "section": "Saving datasets",
    "text": "Saving datasets\nYou can save datasets to many different formats, e.g., back to .csv, to Stata or SAS files, excel files, and many many more. There is an R package for almost all file formats you might need. If you share data and don’t mind large files, csv is not a bad choice (e.g., using readr::write_csv). Do not save datsets to excel! Excel files have a row limit that is often exceeded and the chance that data formats and similar things get garbled is just too high. ({writexl} is a package including functions for writing excel files. {readxl} for reading).\nR also has a way store datasets into binary format. These files are called .rds files. They are reasonably fast and compact, so not a bad choice for saving intermediate output that is not supposed to be stored permanently. We will use it now.\n\nsaveRDS(ccm_unique, \"data/ccm-unique.rds\")\n# readRDS for reading the file in again\n\nWe will continue preparing the sample in the second part of the workshop “Transforming Data”."
  },
  {
    "objectID": "data.html#plotting-to-understand-data",
    "href": "data.html#plotting-to-understand-data",
    "title": "Part 1",
    "section": "Plotting to understand data",
    "text": "Plotting to understand data\nA good plotting library is an incredibly powerful tool to not only present results but also to explore and understand data. See for example the BBC Visual and Data Journalism cookbook for R graphics for some great examples of good plots. In our case, we wan to get a better feeling for how the distribution of firm size in our dataset.\nWe could start with a simple histogram\n\nccm_unique |&gt; \n1  drop_na(at) |&gt;\n2  ggplot(aes(x = at)) +\n3  geom_histogram(bins = 50)\n\n\n1\n\nDrop all rows with missing values of at (assets total) from the data frame\n\n2\n\nCreate a plotting canvas with the x axis being mapped to at\n\n3\n\nDraw a histogram on the plot canvas using 50 bins\n\n\n\n\n\n\n\n\n\n\n\nUnfortunately, we do not see much here. The reason is the extremely skewed distribution of variables like total assets, sales, household income. There is always a “Apple Inc.” that is so much larger, or a “Jeff Bezoz” that is so much richer than everyone else. For variables that can only have positive values, taking the logarithm can yield in a much easier visualization. But we need a to be a bit careful with our log-taking. If we have negative values, the log() function will produce missing values and if we take the log of zero we get “negative infinity” as a result\n\nccm_unique |&gt; \n1  mutate(Size = log(at)) |&gt;\n2  select(at, Size) |&gt;\n3  qsu()\n\n\n1\n\nMutate the data by taking the log of at and storing the result into a new column named “Size”\n\n2\n\nOnly keep the columns at and Size. Drop all other columns\n\n3\n\ncollapse:qsu computes a quick summary (qsu)\n\n\n\n\n           N       Mean          SD   Min       Max\nat    218211  4761.9387  47680.2673     0  3'743567\nSize  218211          -           -  -Inf   15.1355\n\n\nThe -Inf is a problem when plotting so we filter it out before\n\nccm_unique |&gt; \n1  mutate(Size = log(at)) |&gt;\n2  filter(is.finite(Size)) |&gt;\n3  ggplot(aes(x = Size)) +\n4  geom_histogram(bins = 50)\n\n\n1\n\nMutate the data by taking the log of at and storing the result into a new column named “Size”\n\n2\n\nDrop all rows with infinite values of Size from the data frame\n\n3\n\nCreate a plotting canvas with the x axis being mapped to Size\n\n4\n\nDraw a histogram on the plot canvas using 50 bins\n\n\n\n\n\n\n\n\n\n\n\nThis looks much better. However, this is the distribution over all years in our sample. It might hide interesting patterns in the data. We could take a look at how the data changed over time. There is multiple ways we could try to visualize and explore such changes. Here is one. For this we need a third extra package called {ggrides}\n\nlibrary(ggridges)\n\nIf you are surprised that you need an yet another package to run the code, you should be. As we said above, this is bad coding practice. Put all your library calls always at the top of your scripts, so that everyone can always see what is required to run all code. ggridges contains functions to plot so-called ridge plots. We will use ggridges::geom_density_ridges for that.\n\n1fig1 &lt;-\n  ccm_unique |&gt;                                          \n2  mutate(Size = log(at)) |&gt;\n  filter(                                                  \n3    is.finite(Size),\n4    !is.na(fyear) & fyear &lt; 2023\n  ) |&gt; \n5  ggplot(aes(x = Size, y = as.factor(fyear))) +\n6  geom_density_ridges(\n    scale = 5, \n    fill = \"coral\", color = \"black\",\n    alpha = 0.2, \n    rel_min_height = 0.005\n  ) +\n7  geom_vline(xintercept = 5, color = \"grey30\") +\n8  scale_y_discrete(expand = c(0.01, 0)) +\n9  scale_x_continuous(expand = c(0.01, 0)) +\n10  theme_light() +\n11  theme(panel.grid.minor = element_blank()) +\n12  labs(\n    y = \"Fiscal Year\",\n    x = \"Firm Size (log of total assets)\",\n    title = \"Listed firms are much bigger today\",\n    subtitle = \"Change in firm size distribution over time\",\n    caption = \"Source: Compustat North America data (1980 - 2022). Not adjusted for inflation\"\n  )\n\n\n1\n\nStore the result of the computation into variable called fig1\n\n2\n\nMutate the data by taking the log of at and storing the result into a new column named “Size”\n\n3\n\nDrop all rows with infinite values of Size from the data frame\n\n4\n\nDrop all rows with missing fiscal year (fyear) and fyear greater equal 2023\n\n5\n\nCreate a plotting canvas with the x axis being mapped to Size and the y axis to fyear where fyear is transformed from an integer variable to a factor before. (A quirk of ggridges makes this necessary)\n\n6\n\nDraw a ridge plot on the plot canvas using a certain parameter configuration\n\n7\n\nAdd a guiding vertical line that intersects the x axis at 5.\n\n8\n\nReduce the padding around the x axis (The default padding is to much for my taste)\n\n9\n\nReduce the padding around the y axis\n\n10\n\nSwitch the standard grey theme to a lighter theme\n\n11\n\nRemove some unnecessary gridlines\n\n12\n\nAdd annotations to finalize the plot\n\n\n\n\nWe have stored the plot into a variable called fig1 instead of plotting it directly. This is useful for post-processing, saving, etc. Here is the plot in fig1\n\nfig1\n\nPicking joint bandwidth of 0.361\n\n\n\n\n\nAnd here is how to save it to disk in .png format (good for websites)\n\nggsave(\"size-by-year.png\", fig1, width = 7, height = 7, units = \"in\")\n\nPicking joint bandwidth of 0.361"
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese are essentially text files that you could open in a text editor to look at the values. They have the advantage that most programs can read them (because they are text files). The disadvantage is that they usually take more disk space and take longer to read. That is something you will only notice with big files though.↩︎\nBase R also has a data.frame type, of which tibble is a derivative. There are also others, like data.table (which we won’t cover, even though it is awesome. It is is advanced stuff). Except for data.table, they are mostly interchangeable.↩︎"
  }
]